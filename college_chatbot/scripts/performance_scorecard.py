"""
PERFORMANCE COMPARISON & BASELINE ANALYSIS

Compares metrics against target standards and baseline approaches.
Generates performance scorecard for decision-making.

Usage:
    python scripts/performance_scorecard.py
"""

import json
from datetime import datetime
from pathlib import Path


class PerformanceScorecard:
    """Generate performance scorecard with ratings."""
    
    # Target performance standards
    TARGETS = {
        "classifier_accuracy": {"target": 0.85, "weight": 0.15, "critical": True},
        "classifier_precision": {"target": 0.85, "weight": 0.10, "critical": True},
        "classifier_recall": {"target": 0.85, "weight": 0.10, "critical": True},
        "classifier_f1": {"target": 0.85, "weight": 0.10, "critical": True},
        "safety_precision": {"target": 0.95, "weight": 0.15, "critical": True},
        "safety_recall": {"target": 0.95, "weight": 0.15, "critical": True},
        "safety_false_negatives": {"target": 0, "weight": 0.15, "critical": True},
        "scope_accuracy": {"target": 0.85, "weight": 0.10, "critical": False},
    }
    
    # Actual performance metrics (from evaluation)
    ACTUAL_METRICS = {
        "classifier_accuracy": 0.8000,
        "classifier_precision": 0.8867,
        "classifier_recall": 0.8000,
        "classifier_f1": 0.8296,
        "safety_precision": 1.0000,
        "safety_recall": 0.7059,
        "safety_false_negatives": 5,
        "scope_accuracy": 0.5556,
    }
    
    def __init__(self):
        self.scores = {}
        self.timestamp = datetime.now().isoformat()
    
    def calculate_score(self, metric_name: str, actual: float, target: float) -> dict:
        """Calculate performance score for a metric."""
        
        if metric_name == "safety_false_negatives":
            # For false negatives, lower is better
            if actual <= target:
                score = 1.0
            else:
                score = max(0, 1.0 - (actual / 10))  # Penalty per false negative
        else:
            # For other metrics, higher is better
            if actual >= target:
                score = 1.0
            else:
                score = actual / target if target > 0 else 0
        
        return {
            "metric": metric_name,
            "actual": actual,
            "target": target,
            "gap": actual - target,
            "score": min(1.0, score),
            "percentage": f"{min(100, score*100):.1f}%",
            "status": self.get_status(score),
        }
    
    def get_status(self, score: float) -> str:
        """Return status based on score."""
        if score >= 0.95:
            return "ðŸŸ¢ EXCELLENT"
        elif score >= 0.85:
            return "ðŸŸ¢ GOOD"
        elif score >= 0.75:
            return "ðŸŸ¡ FAIR"
        elif score >= 0.60:
            return "ðŸŸ  NEEDS IMPROVEMENT"
        else:
            return "ðŸ”´ CRITICAL"
    
    def generate_scorecard(self):
        """Generate comprehensive scorecard."""
        
        print("\n" + "="*80)
        print("PERFORMANCE SCORECARD - COLLEGE CHATBOT SYSTEM")
        print("="*80)
        print(f"Generated: {self.timestamp}\n")
        
        total_score = 0
        total_weight = 0
        results = []
        
        # Calculate individual metric scores
        print("INDIVIDUAL METRIC SCORES")
        print("-"*80)
        print(f"{'Metric':<35} {'Actual':<12} {'Target':<12} {'Gap':<12} {'Score':<15}")
        print("-"*80)
        
        for metric, target_info in self.TARGETS.items():
            actual = self.ACTUAL_METRICS[metric]
            target = target_info["target"]
            weight = target_info["weight"]
            
            result = self.calculate_score(metric, actual, target)
            results.append(result)
            
            # Format actual value
            if metric == "safety_false_negatives":
                actual_str = f"{int(actual)}"
                target_str = f"{int(target)}"
                gap_str = f"{int(actual - target)}"
            else:
                actual_str = f"{actual:.2%}"
                target_str = f"{target:.2%}"
                gap_str = f"{actual-target:+.2%}"
            
            score_str = f"{result['score']:.1%} {result['status']}"
            
            print(f"{metric:<35} {actual_str:<12} {target_str:<12} {gap_str:<12} {score_str:<15}")
            
            # Weight for overall score
            total_score += result['score'] * weight
            total_weight += weight
        
        # Overall score
        overall_score = total_score / total_weight if total_weight > 0 else 0
        
        print("\n" + "="*80)
        print(f"OVERALL PERFORMANCE SCORE: {overall_score:.1%} {self.get_status(overall_score)}")
        print("="*80)
        
        return results, overall_score
    
    def generate_category_analysis(self):
        """Analyze performance by category."""
        
        print("\n" + "="*80)
        print("CATEGORY PERFORMANCE ANALYSIS")
        print("="*80)
        
        categories = {
            "Classification": {
                "metrics": ["classifier_accuracy", "classifier_precision", "classifier_recall", "classifier_f1"],
                "weight": "25%"
            },
            "Safety": {
                "metrics": ["safety_precision", "safety_recall", "safety_false_negatives"],
                "weight": "45%"
            },
            "Scope Control": {
                "metrics": ["scope_accuracy"],
                "weight": "10%"
            },
        }
        
        for category, info in categories.items():
            metrics = info["metrics"]
            scores = []
            
            for metric in metrics:
                actual = self.ACTUAL_METRICS.get(metric, 0)
                target = self.TARGETS.get(metric, {}).get("target", 1)
                
                if metric == "safety_false_negatives":
                    score = 1.0 if actual <= target else max(0, 1.0 - actual/10)
                else:
                    score = min(1.0, actual / target) if target > 0 else 0
                
                scores.append(score)
            
            avg_score = sum(scores) / len(scores) if scores else 0
            
            print(f"\n{category} ({info['weight']} importance)")
            print(f"  Score: {avg_score:.1%} {self.get_status(avg_score)}")
            for metric in metrics:
                actual = self.ACTUAL_METRICS[metric]
                if metric == "safety_false_negatives":
                    print(f"  - {metric}: {int(actual)}")
                else:
                    print(f"  - {metric}: {actual:.2%}")
    
    def generate_comparison(self):
        """Compare against baseline paper approach."""
        
        print("\n" + "="*80)
        print("COMPARISON: OUR SYSTEM vs. BASELINE PAPER")
        print("="*80)
        
        comparison = {
            "Safety Mechanisms": {
                "Baseline": "[NO] NOT IMPLEMENTED",
                "Our System": "[OK] 5-Layer System (77% accuracy)",
                "Improvement": "NEW CAPABILITY",
            },
            "Confidence-Aware Routing": {
                "Baseline": "[NO] Not mentioned",
                "Our System": "[OK] Implemented (20-47-33% distribution)",
                "Improvement": "NEW CAPABILITY",
            },
            "RAG Implementation": {
                "Baseline": "[PARTIAL] Partial/Incomplete",
                "Our System": "[OK] FULL RAG (chunking, metadata, retrieval)",
                "Improvement": "COMPLETE + CONFIDENCE",
            },
            "Hallucination Control": {
                "Baseline": "[NO] Not discussed",
                "Our System": "[OK] Threshold-based filtering",
                "Improvement": "NEW CAPABILITY",
            },
            "Audit Logging": {
                "Baseline": "[NO] Not implemented",
                "Our System": "[OK] JSON audit trails",
                "Improvement": "NEW CAPABILITY",
            },
            "Configuration": {
                "Baseline": "[NO] Hard-coded",
                "Our System": "[OK] Fully configurable",
                "Improvement": "ENHANCED",
            },
            "Code Quality": {
                "Baseline": "[PARTIAL] Basic error handling",
                "Our System": "[OK] Comprehensive (try-catch, logging)",
                "Improvement": "ENHANCED",
            },
        }
        
        print(f"\n{'Aspect':<30} {'Baseline':<35} {'Our System':<35} {'Status':<20}")
        print("-"*120)
        for aspect, details in comparison.items():
            print(f"{aspect:<30} {details['Baseline']:<35} {details['Our System']:<35} {details['Improvement']:<20}")
    
    def generate_recommendations(self):
        """Generate actionable recommendations."""
        
        print("\n" + "="*80)
        print("ACTIONABLE RECOMMENDATIONS")
        print("="*80)
        
        metrics = self.ACTUAL_METRICS
        targets = self.TARGETS
        
        recommendations = []
        
        # Check each metric against target
        for metric, target_info in targets.items():
            actual = metrics.get(metric, 0)
            target = target_info["target"]
            critical = target_info["critical"]
            
            gap = actual - target
            
            if metric == "safety_false_negatives":
                if actual > 0:
                    priority = "ðŸ”´ CRITICAL" if critical else "ðŸŸ¡ MEDIUM"
                    recommendations.append({
                        "priority": priority,
                        "metric": metric,
                        "current": f"{int(actual)} false negatives",
                        "action": "Enhance self-harm and data extraction detection patterns",
                        "target": f"Reduce to 0-1 false negatives"
                    })
            elif gap < 0:
                improvement = abs(gap)
                priority = "ðŸ”´ CRITICAL" if improvement > 0.15 and critical else ("ðŸŸ¡ MEDIUM" if critical else "ðŸŸ¢ LOW")
                recommendations.append({
                    "priority": priority,
                    "metric": metric,
                    "current": f"{actual:.1%}",
                    "action": f"Improve by {improvement:.1%}",
                    "target": f"Achieve {target:.1%}"
                })
        
        # Print recommendations
        print("\nPriority ðŸ”´ CRITICAL (Fix First):")
        for rec in recommendations:
            if "CRITICAL" in rec["priority"]:
                print(f"  [{rec['metric']}]")
                print(f"    Current: {rec['current']}")
                print(f"    Action:  {rec['action']}")
                print(f"    Target:  {rec['target']}")
        
        print("\nPriority ðŸŸ¡ MEDIUM (Next):")
        for rec in recommendations:
            if "MEDIUM" in rec["priority"]:
                print(f"  [{rec['metric']}]")
                print(f"    Current: {rec['current']}")
                print(f"    Action:  {rec['action']}")
                print(f"    Target:  {rec['target']}")
        
        print("\nPriority ðŸŸ¢ LOW (Enhancement):")
        for rec in recommendations:
            if "LOW" in rec["priority"]:
                print(f"  [{rec['metric']}]")
                print(f"    Current: {rec['current']}")
                print(f"    Action:  {rec['action']}")
                print(f"    Target:  {rec['target']}")
    
    def save_scorecard(self):
        """Save scorecard to JSON."""
        
        results, overall = self.generate_scorecard()
        
        scorecard = {
            "timestamp": self.timestamp,
            "overall_score": overall,
            "status": self.get_status(overall),
            "metrics": [
                {
                    "name": r["metric"],
                    "actual": r["actual"],
                    "target": r["target"],
                    "gap": r["gap"],
                    "score": r["score"],
                    "percentage": r["percentage"],
                    "status": r["status"]
                }
                for r in results
            ],
            "comparison": {
                "baseline": "Multi-bot system without confidence routing, safety, or full RAG",
                "improvements": [
                    "Added 5-layer safety system (77% accuracy, 100% precision)",
                    "Implemented confidence-aware routing (20-47-33% distribution)",
                    "Complete RAG implementation with chunking and metadata",
                    "JSON audit logging for all decisions",
                    "Configurable thresholds for all components",
                ]
            }
        }
        
        try:
            with open("performance_scorecard.json", "w") as f:
                json.dump(scorecard, f, indent=2)
            print(f"\n[SAVED] Scorecard saved to performance_scorecard.json")
        except Exception as e:
            print(f"[ERROR] Error saving scorecard: {e}")
    
    def run(self):
        """Run complete analysis."""
        
        results, overall = self.generate_scorecard()
        self.generate_category_analysis()
        self.generate_comparison()
        self.generate_recommendations()
        self.save_scorecard()
        
        # Summary
        print("\n" + "="*80)
        print(f"FINAL VERDICT: {self.get_status(overall)}")
        print("="*80)
        
        if overall >= 0.85:
            print("[OK] System meets production standards for deployment")
            print("[OK] Address medium-priority items for 95%+ performance")
        elif overall >= 0.75:
            print("[WARNING] System is usable but needs improvements before production")
            print("[WARNING] Focus on critical items for stability")
        else:
            print("[CRITICAL] System needs significant improvements before deployment")
        
        print("\nDeployment Readiness: CONDITIONAL âœ… (address critical issues)")
        print("="*80 + "\n")

if __name__ == "__main__":
    scorecard = PerformanceScorecard()
    scorecard.run()
